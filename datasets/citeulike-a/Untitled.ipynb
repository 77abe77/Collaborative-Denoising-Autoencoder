{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "path = 'users.dat'\n",
    "\n",
    "input_vect_size = 0\n",
    "with open(path, 'rb') as input_file:\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        line = line.split()[1:]\n",
    "        for item in line:\n",
    "            if int(item) > input_vect_size:\n",
    "                input_vect_size = int(item)\n",
    "            \n",
    "\n",
    "with open(path, 'rb') as input_file:\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        line = line.split()\n",
    "        #num_saved_items = int(line[0])\n",
    "        #indices = []\n",
    "        \n",
    "        full_data = []\n",
    "        for i in range(1, len(line)):\n",
    "            full_data.append(int(i))\n",
    "        training_data = get_training_data(np.array(full_data))\n",
    "        val_set.append(make_sparse_from_raw_set(full_data, input_vect_size))\n",
    "        train_set.append(make_sparse_from_raw_set(training_data, input_vect_size))\n",
    "        \n",
    "            #indices.append([0, int(line[i]) - 1])\n",
    "        #values = [1] * num_saved_items\n",
    "        #shape = [1, input_vect_size]\n",
    "        #sv = tf.SparseTensor(indices=indices, values=values, shape=shape)\n",
    "\n",
    "# Need to get the sampled on from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sess = tf.Session()\n",
    "dv = tf.sparse_tensor_to_dense(sv)\n",
    "res = sess.run(dv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.where(res > 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sparse_from_raw_set(raw_set, size):\n",
    "    indices = [[0, i - 1] for i in raw_set]\n",
    "    values = [1] * len(indices)\n",
    "    shape = [1, size]\n",
    "    #return [indices, values, shape]\n",
    "    return tf.SparseTensor(indices=indices, values=values, shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(full_dataset_one_d):\n",
    "    '''Input: 1-D vector of just the item numbers that are included in\n",
    "        in the full dataset\n",
    "       Output: 2-D with the first dimension holding the test set that\n",
    "               omits 20% of the data, and the second dimension including\n",
    "               the full set. In this application the validation set is\n",
    "               the full set.\n",
    "    '''\n",
    "    #TODO - Might need to check that training set statisfies a min length\n",
    "    full_set = list(full_dataset_one_d)\n",
    "    ind = np.random.randint(1, len(full_dataset_one_d), len(full_dataset_one_d))\n",
    "    train_set_ind = np.where(ind <= 0.8 * len(full_dataset_one_d))[0]\n",
    "    train_set = full_set[train_set_ind]\n",
    "    \n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b, c = get_validation_set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1630,  2316,  2525,  2845,  2930,  3170,  3296,  3331,  3403,\n",
       "        3594,  3769,  3949,  4661,  4870,  4888,  5113,  5323,  5324,\n",
       "        5613,  5990,  6102,  6873,  6967,  7105,  7800,  7866,  8902,\n",
       "        9906, 10007, 10203, 10287, 10507, 10587, 11008, 11104, 11225,\n",
       "       11852, 11918, 12683, 12715, 12801, 12803, 12804, 12830, 12915,\n",
       "       13171, 13362, 13922, 13923, 14661, 14675, 14850, 15025, 15164,\n",
       "       15190, 15281, 15299, 15335, 15832, 15893, 16162, 16423])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885714285714\n"
     ]
    }
   ],
   "source": [
    "len_b = b.shape[0]\n",
    "len_a = a.shape[0]\n",
    "print float(len_b) / len_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = make_sparse_vect_from_raw_set(b,input_vect_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.SparseTensor at 0x7f55c47e4550>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = lambda i: [i]\n",
    "ex(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.Variable([1,1,1])\n",
    "b = tf.Variable([2,2,2])\n",
    "#tf.pack([a, b])\n",
    "#tf.Variable([[1,2]])\n",
    "res = tf.matmul(tf.Variable([[1,2]]), [a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 5, 5]], dtype=int32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_init_val = [tf.constant(range(i, i+2), dtype=tf.float32) for i in range(1, 7, 2)]\n",
    "W_prime_init_val = [tf.constant(range(i, i+2), dtype=tf.float32) for i in range(7, 13, 2)]\n",
    "b_prime_init_val = [tf.constant(2.0, dtype=tf.float32) for _ in xrange(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "l2_normalize() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-6603143de858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#z_out, y_hat_out, loss, grad_w0, grad_w_prime0, grad_w1, grad_w_prime1 = sess.run((z, y_hat, loss_0, dw_0, dw_prime_0, dw_1, dw_prime_1), feed_dict={y: y_0})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mnormalized_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mw_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: l2_normalize() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "y_0 = [[0, 1, 1]]\n",
    "y = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "# Test Gradients\n",
    "weight = dict()\n",
    "weight['W'] = [tf.Variable(seq) for seq in W_init_val]\n",
    "weight['W_prime'] = [tf.Variable(seq) for seq in W_prime_init_val]\n",
    "weight['b'] = tf.Variable(tf.constant([[1.0, 1.0]], dtype=tf.float32))\n",
    "weight['b_prime'] = [tf.Variable(seq) for seq in b_prime_init_val]\n",
    "z = tf.add(tf.matmul(y, weight['W']), weight['b'])\n",
    "y_hat = tf.add(tf.matmul(z, tf.transpose(weight['W_prime'])), weight['b_prime'])\n",
    "\n",
    "loss_0 = tf.nn.l2_loss(tf.sub(y, y_hat))\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "W_var_before = [sess.run(var) for var in weight['W']]\n",
    "W_prime_var_before = [sess.run(var) for var in weight['W_prime']]\n",
    "\n",
    "#Gradient\n",
    "#dw_0 = tf.gradients(loss_0, [weight['W'][0]])[0]\n",
    "#dw_prime_0 = tf. gradients(loss_0, [weight['W_prime'][0]])[0]\n",
    "dw_prime_0 = optimizer.minimize(loss_0, var_list=[weight['W_prime'][0]])\n",
    "#dw_1 = tf.gradients(loss_0, [weight['W'][1]])[0]\n",
    "#dw_prime_1 = tf. gradients(loss_0, [weight['W_prime'][1]])[0]\n",
    "sess.run(dw_prime_0, feed_dict={y: y_0})\n",
    "W_var = [sess.run(var) for var in weight['W']]\n",
    "W_prime_var = [sess.run(var) for var in weight['W_prime']]\n",
    "#z_out, y_hat_out, loss, grad_w0, grad_w_prime0, grad_w1, grad_w_prime1 = sess.run((z, y_hat, loss_0, dw_0, dw_prime_0, dw_1, dw_prime_1), feed_dict={y: y_0})\n",
    "\n",
    "normalized_weights = [tf.nn.l2_normalize(var, 0) for var in weight.iteritems()]\n",
    "w_sum = tf.reduce_sum(normalized_weights)\n",
    "\n",
    "reg_sum = sess.run(w_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(W_prime_var[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "y_0 = [[1, 0, 1]]\n",
    "y = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "# Test Gradients\n",
    "weight = dict()\n",
    "weight['W'] = [tf.Variable(seq) for seq in W_init_val]\n",
    "weight['W_prime'] = [tf.Variable(seq) for seq in W_prime_init_val]\n",
    "weight['b'] = tf.Variable(tf.constant([[1.0, 1.0]], dtype=tf.float32))\n",
    "weight['b_prime'] = [tf.Variable(seq) for seq in b_prime_init_val]\n",
    "z = tf.add(tf.matmul(y, weight['W']), weight['b'])\n",
    "y_hat = tf.add(tf.matmul(z, tf.transpose(weight['W_prime'])), weight['b_prime'])\n",
    "\n",
    "loss_0 = tf.nn.l2_loss(tf.sub(y, y_hat))\n",
    "#loss_1 = tf.nn.l2_loss(tf.sub(y, y_hat))\n",
    "#cost = 0.5 * tf.reduce_sum(tf.nn.l2_loss(tf.sub(y, y_hat)))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "W_var_before = [sess.run(var) for var in weight['W']]\n",
    "W_prime_var_before = [sess.run(var) for var in weight['W_prime']]\n",
    "\n",
    "#Gradient\n",
    "#dw_0 = tf.gradients(loss_0, [weight['W'][0]])[0]\n",
    "#dw_prime_0 = tf. gradients(loss_0, [weight['W_prime'][0]])[0]\n",
    "dw_prime_0 = optimizer.minimize(loss_0, var_list=[weight['W_prime'][0]])\n",
    "#dw_1 = tf.gradients(loss_0, [weight['W'][1]])[0]\n",
    "#dw_prime_1 = tf. gradients(loss_0, [weight['W_prime'][1]])[0]\n",
    "#sess.run(dw_prime_0, feed_dict={y: y_0})\n",
    "#W_var = [sess.run(var) for var in weight['W']]\n",
    "#W_prime_var = [sess.run(var) for var in weight['W_prime']]\n",
    "#z_out, y_hat_out, loss, grad_w0, grad_w_prime0, grad_w1, grad_w_prime1 = sess.run((z, y_hat, loss_0, dw_0, dw_prime_0, dw_1, dw_prime_1), feed_dict={y: y_0})\n",
    "\n",
    "#normalized_weights = [tf.nn.l2_normalize(var, 0) for var in weight.iteritems()]\n",
    "#w_sum = tf.reduce_sum(normalized_weights)\n",
    "\n",
    "#reg_sum = sess.run(w_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = 0.1\n",
    "loss = tf.nn.l2_loss(tf.sub(y, y_hat))\n",
    "\n",
    "dloss_d = lambda var: optimizer.compute_gradients(loss, var_list=[var])[0][0]\n",
    "dCost_d = lambda var: dloss_d(var) - tf.mul(l, var)\n",
    "\n",
    "grad_op = lambda var: optimizer.apply_gradients([[dCost_d(var), var]])\n",
    "W_var_before = [sess.run(var) for var in weight['W']]\n",
    "#cost = lambda var: loss - tf.mul(l, var)\n",
    "#cost = 0.5 * tf.reduce_sum([tf.nn.l2_loss(vect) for vect in tf.sub(z_0, z)])\n",
    "#cost = 0.5 * tf.reduce_sum(tf.nn.l2_loss(tf.sub(z_0, z)))\n",
    "sess.run(grad_op(weight['W'][0]), feed_dict={y: y_0})\n",
    "W_var = [sess.run(var) for var in weight['W']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.,  2.], dtype=float32),\n",
       " array([ 3.,  4.], dtype=float32),\n",
       " array([ 5.,  6.], dtype=float32)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_var_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-41.9489975, -45.5779953], dtype=float32),\n",
       " array([ 3.,  4.], dtype=float32),\n",
       " array([ 5.,  6.], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
